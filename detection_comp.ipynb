{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4fbd3a-ee02-45c5-803f-4b6355459209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, torch, sys\n",
    "# allows us to import from the custom configs directory w/o affecting deepdisc library imports\n",
    "sys.path.insert(0, '/u/yse2/deepdisc/configs')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import FoFCatalogMatching\n",
    "import pycocotools.mask as mask_util\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "# for unrec blend\n",
    "from astropy.coordinates import search_around_sky, SkyCoord\n",
    "from astropy.visualization import make_lupton_rgb\n",
    "from astropy.wcs import WCS\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "import pycocotools.mask as mask_util\n",
    "# detectron2 and deepdisc\n",
    "from detectron2.config import LazyConfig\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import Instances, Boxes\n",
    "from deepdisc.astrodet.visualizer import Visualizer, ColorMode\n",
    "from deepdisc.data_format.register_data import register_data_set\n",
    "from custom.mappers import FileNameWCSMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf2c21c-5e73-47ee-a4c1-4319bb4532d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test data from /u/yse2/lsst_data/annotations_lvl5/test_8k.json with 8571 images.\n",
      "Run name: lsst5_30k_4h200_bs192_ep50 and run dir: /u/yse2/lsst_runs/lsst5_30k_4h200_bs192_ep50/\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.expanduser('~/lsst_data/')\n",
    "root_run_dir = os.path.expanduser('~/lsst_runs/')\n",
    "folder = 'annotations_lvl5'\n",
    "test_data_fn = f'{root_dir}{folder}/test_8k.json'\n",
    "test_cats_dir = f'{root_dir}test_cats_lvl5/test_8k/'\n",
    "test_data = pd.read_json(test_data_fn)\n",
    "run_name = 'lsst5_30k_4h200_bs192_ep50'\n",
    "run_dir = f'{root_run_dir}{run_name}/'\n",
    "print(f\"Loaded test data from {test_data_fn} with {len(test_data)} images.\")\n",
    "print(f\"Run name: {run_name} and run dir: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08080da-713f-4012-b10a-504d3ba5dff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully!\n",
      "Registering training dataset from: /u/yse2/lsst_data/annotations_lvl5/test_8k.json\n",
      "Dataset registered successfully!\n",
      "Test Score Threshold: 0.25\n",
      "NMS Threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "cfg_file = os.path.expanduser(\"~/deepdisc/configs/solo/swin_lsst_job.py\")\n",
    "cfg = LazyConfig.load(cfg_file) # using a Swin Transformer\n",
    "for key in cfg.get(\"MISC\", dict()).keys():\n",
    "    cfg[key] = cfg.MISC[key]\n",
    "\n",
    "cfg.DATASETS.TEST = \"test\"\n",
    "cfg.dataloader.augs = None # no augs for test set since we want preds on OG images\n",
    "cfg.dataloader.test.mapper = FileNameWCSMapper # setting test DataLoader's mapper so that filename gets added to each sample\n",
    "print(f\"Config loaded successfully!\")\n",
    "print(f\"Registering training dataset from: {test_data_fn}\")\n",
    "try:\n",
    "    DatasetCatalog.remove(cfg.DATASETS.TEST)\n",
    "    MetadataCatalog.remove(cfg.DATASETS.TEST)\n",
    "except:\n",
    "    pass\n",
    "custom_colors = [\n",
    "    (0, 255, 0),    # green for galaxies\n",
    "    (0, 0, 255),    # blue for stars\n",
    "]\n",
    "astrotest_metadata = register_data_set(\n",
    "    cfg.DATASETS.TEST, test_data_fn, thing_classes=cfg.metadata.classes, thing_colors=custom_colors\n",
    ")\n",
    "test_score_thresh = 0.25\n",
    "nms_thresh = 0.5\n",
    "print(f\"Dataset registered successfully!\")\n",
    "print(f\"Test Score Threshold: {test_score_thresh}\")\n",
    "print(f\"NMS Threshold: {nms_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d6108f-beb2-4084-b331-a65311fb4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffers = [1, 2]\n",
    "mag_limits = {\n",
    "    'power_law': 26.07,\n",
    "    'gold': 25.3,\n",
    "    'nominal': 26.42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de30b457-d7a4-4f4b-814f-2cecd747d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LSST truth catalog from: /u/yse2/lsst_data/test_cats_lvl5/test_8k/test_truth_cat_maglim_26.30.parquet\n"
     ]
    }
   ],
   "source": [
    "truth_mag_limit = mag_limits['gold'] + buffers[0]\n",
    "truth_fn = f'{test_cats_dir}test_truth_cat_maglim_{truth_mag_limit:.2f}.parquet'\n",
    "# truth_fn = f'/u/yse2/lsst_runs/{run_name}/test_cats/full_test_truth_cat.parquet'\n",
    "print(f'Loading LSST truth catalog from: {truth_fn}')\n",
    "lsst_truth_cat = pd.read_parquet(truth_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b001b3e-596b-45d1-aea2-1dc474c37e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fn = f'{run_dir}preds/pred_with_mag_s{test_score_thresh}_n{nms_thresh}.json'\n",
    "with open(pred_fn, 'r') as f:\n",
    "    dd_det = json.load(f)\n",
    "dd_det_cat = pd.DataFrame(dd_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7154a735-5dea-42bd-a0d8-de66ea562e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_det_cat = pd.read_json(f'{test_cats_dir}test_det_cat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f0278b-d8f2-4288-8585-a435f5782b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip here to re-load saved results (but make sure to set truth_mag_limit correctly)\n",
    "analysis = {}\n",
    "counts = {}\n",
    "all_matches = {\n",
    "    '0.5': {},\n",
    "    '1.0': {}\n",
    "}\n",
    "for ll in ['0.5', '1.0']:\n",
    "    analysis[ll] = pd.read_parquet(f'{os.path.expanduser(\"~\")}/lsst_runs/{run_name}/analysis{ll}_{truth_mag_limit}.parquet')\n",
    "    counts[ll] = pd.read_parquet(f'{run_dir}grp_class{ll}_{truth_mag_limit}.parquet')\n",
    "    all_matches[ll]['dd'] = pd.read_parquet(f'{run_dir}obj_matches_dd{ll}_{truth_mag_limit}.parquet')\n",
    "    all_matches[ll]['lsst'] = pd.read_parquet(f'{run_dir}obj_matches_lsst{ll}_{truth_mag_limit}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d64f6e-44df-4f82-846a-8a6b93f20a91",
   "metadata": {},
   "source": [
    "# Object-Level Detection Completeness\n",
    "Now that we have all object-level match records, we can calculate detection completeness\n",
    "metrics. Object-level completeness measures the fraction of individual truth objects\n",
    "that were successfully detected and matched. We calculate the fraction of truth objects that were successfully matched to detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef7999a-99d1-42d2-9e43-a55c52f02b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_object_completeness(object_matches, prefix='dd'):\n",
    "    \"\"\"\n",
    "    Calculate object-level detection completeness (recall).\n",
    "    \n",
    "    Args:\n",
    "        object_matches: DataFrame with object-level match records\n",
    "        prefix: 'dd' or 'lsst'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Completeness statistics\n",
    "    \"\"\"\n",
    "    # Filter to truth objects only (exclude spurious detections where truth_row_idx == -1)\n",
    "    truth_objects = object_matches[object_matches['truth_row_idx'] != -1].copy()\n",
    "    # Each truth object should appear exactly once in the match records\n",
    "    # (either matched or unmatched)\n",
    "    truth_objects_unique = truth_objects.drop_duplicates(subset='truth_row_idx')\n",
    "    \n",
    "    # Count matched vs total\n",
    "    total_truth = len(truth_objects_unique)\n",
    "    matched_truth = truth_objects_unique['matched'].sum()\n",
    "    unmatched_truth = total_truth - matched_truth\n",
    "    \n",
    "    completeness = matched_truth / total_truth if total_truth > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_truth': total_truth,\n",
    "        'matched_truth': matched_truth,\n",
    "        'unmatched_truth': unmatched_truth,\n",
    "        'completeness': completeness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a17cad32-dfaf-4fd6-a77d-237eb490be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECT-LEVEL DETECTION COMPLETENESS\n",
      "\n",
      "============================================================\n",
      "Linking Length 0.5\"\n",
      "============================================================\n",
      "\n",
      "--- DeepDISC Completeness ---\n",
      "Total truth objects:     224,192\n",
      "Matched truth objects:   182,507\n",
      "Unmatched truth objects: 41,685\n",
      "Completeness (Recall):   81.41%\n",
      "\n",
      "--- LSST Pipeline Completeness ---\n",
      "Total truth objects:     224,192\n",
      "Matched truth objects:   185,678\n",
      "Unmatched truth objects: 38,514\n",
      "Completeness (Recall):   82.82%\n",
      "\n",
      "--- Comparison ---\n",
      "DeepDISC advantage: -1.41% (-1.4 percentage points)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Linking Length 1.0\"\n",
      "============================================================\n",
      "\n",
      "--- DeepDISC Completeness ---\n",
      "Total truth objects:     224,192\n",
      "Matched truth objects:   197,391\n",
      "Unmatched truth objects: 26,801\n",
      "Completeness (Recall):   88.05%\n",
      "\n",
      "--- LSST Pipeline Completeness ---\n",
      "Total truth objects:     224,192\n",
      "Matched truth objects:   187,125\n",
      "Unmatched truth objects: 37,067\n",
      "Completeness (Recall):   83.47%\n",
      "\n",
      "--- Comparison ---\n",
      "DeepDISC advantage: +4.58% (+4.6 percentage points)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"OBJECT-LEVEL DETECTION COMPLETENESS\")\n",
    "completeness_results = {}\n",
    "\n",
    "for ll in ['0.5', '1.0']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Linking Length {ll}\\\"\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    completeness_results[ll] = {}\n",
    "    \n",
    "    # DD Completeness\n",
    "    print(f\"--- DeepDISC Completeness ---\")\n",
    "    dd_stats = calculate_object_completeness(all_matches[ll]['dd'], 'dd')\n",
    "    completeness_results[ll]['dd'] = dd_stats\n",
    "    \n",
    "    print(f\"Total truth objects:     {dd_stats['total_truth']:,}\")\n",
    "    print(f\"Matched truth objects:   {dd_stats['matched_truth']:,}\")\n",
    "    print(f\"Unmatched truth objects: {dd_stats['unmatched_truth']:,}\")\n",
    "    print(f\"Completeness (Recall):   {dd_stats['completeness']:.2%}\\n\")\n",
    "    \n",
    "    # LSST Completeness\n",
    "    print(f\"--- LSST Pipeline Completeness ---\")\n",
    "    lsst_stats = calculate_object_completeness(all_matches[ll]['lsst'], 'lsst')\n",
    "    completeness_results[ll]['lsst'] = lsst_stats\n",
    "    \n",
    "    print(f\"Total truth objects:     {lsst_stats['total_truth']:,}\")\n",
    "    print(f\"Matched truth objects:   {lsst_stats['matched_truth']:,}\")\n",
    "    print(f\"Unmatched truth objects: {lsst_stats['unmatched_truth']:,}\")\n",
    "    print(f\"Completeness (Recall):   {lsst_stats['completeness']:.2%}\\n\")\n",
    "    \n",
    "    # Comparison\n",
    "    diff = dd_stats['completeness'] - lsst_stats['completeness']\n",
    "    print(f\"--- Comparison ---\")\n",
    "    print(f\"DeepDISC advantage: {diff:+.2%} ({diff*100:+.1f} percentage points)\\n\")\n",
    "\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138d806-457c-46d8-9397-1859798ade11",
   "metadata": {},
   "source": [
    "# Completeness by Magnitude\n",
    "Calculate completeness separately for different mag bins to understand how detection performance varies with object magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd07909-eb3a-42d7-a82c-cb04598332e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comp_by_mag(object_matches, truth_catalog, prefix='dd', mag_bins=[14, 22, 23, 24, 25, 26, 27, 28]):\n",
    "    \"\"\"\n",
    "    Calculate completeness stratified by magnitude bins.\n",
    "    \n",
    "    Args:\n",
    "        object_matches: DataFrame with object-level match records\n",
    "        truth_catalog: Truth catalog with magnitude information\n",
    "        prefix: 'dd' or 'lsst'\n",
    "        mag_bins: Bin edges for magnitude stratification\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with completeness per magnitude bin\n",
    "    \"\"\"\n",
    "    # Filter to truth objects only\n",
    "    truth_matches = object_matches[object_matches['truth_row_idx'] != -1].copy()\n",
    "    # Drop duplicates to get unique truth objects\n",
    "    truth_matches_unique = truth_matches.drop_duplicates(subset='truth_row_idx')\n",
    "    # Join with truth catalog to get magnitudes\n",
    "    truth_with_mags = truth_matches_unique.merge(\n",
    "        truth_catalog[['mag_i']],\n",
    "        left_on='truth_row_idx',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    # Create magnitude bins\n",
    "    truth_with_mags['mag_bin'] = pd.cut(\n",
    "        truth_with_mags['mag_i'],\n",
    "        bins=mag_bins,\n",
    "        labels=[f\"{mag_bins[i]:.0f}-{mag_bins[i+1]:.0f}\" for i in range(len(mag_bins)-1)],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    # Calculate completeness per bin\n",
    "    completeness_by_bin = []\n",
    "    for mag_bin in truth_with_mags['mag_bin'].cat.categories:\n",
    "        bin_data = truth_with_mags[truth_with_mags['mag_bin'] == mag_bin]\n",
    "        \n",
    "        if len(bin_data) > 0:\n",
    "            n_total = len(bin_data)\n",
    "            n_matched = bin_data['matched'].sum()\n",
    "            completeness = n_matched / n_total\n",
    "            \n",
    "            completeness_by_bin.append({\n",
    "                'mag_bin': mag_bin,\n",
    "                'n_total': n_total,\n",
    "                'n_matched': n_matched,\n",
    "                'completeness': completeness\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(completeness_by_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41c15a00-87a4-4b84-8f0c-48437cbbf381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETENESS BY MAGNITUDE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Linking Length 0.5\"\n",
      "============================================================\n",
      "\n",
      "--- DeepDISC Completeness by Magnitude ---\n",
      "mag_i 14-22: 61.86% (10,125 objects)\n",
      "mag_i 22-23: 75.27% (10,548 objects)\n",
      "mag_i 23-24: 80.86% (20,843 objects)\n",
      "mag_i 24-25: 84.91% (44,782 objects)\n",
      "mag_i 25-26: 86.45% (92,364 objects)\n",
      "mag_i 26-27: 73.86% (45,463 objects)\n",
      "\n",
      "--- LSST Pipeline Completeness by Magnitude ---\n",
      "mag_i 14-22: 97.06% (10,125 objects)\n",
      "mag_i 22-23: 96.10% (10,548 objects)\n",
      "mag_i 23-24: 94.03% (20,843 objects)\n",
      "mag_i 24-25: 90.11% (44,782 objects)\n",
      "mag_i 25-26: 81.93% (92,364 objects)\n",
      "mag_i 26-27: 66.10% (45,463 objects)\n",
      "\n",
      "============================================================\n",
      "Linking Length 1.0\"\n",
      "============================================================\n",
      "\n",
      "--- DeepDISC Completeness by Magnitude ---\n",
      "mag_i 14-22: 72.64% (10,125 objects)\n",
      "mag_i 22-23: 86.20% (10,548 objects)\n",
      "mag_i 23-24: 91.35% (20,843 objects)\n",
      "mag_i 24-25: 93.87% (44,782 objects)\n",
      "mag_i 25-26: 91.70% (92,364 objects)\n",
      "mag_i 26-27: 77.33% (45,463 objects)\n",
      "\n",
      "--- LSST Pipeline Completeness by Magnitude ---\n",
      "mag_i 14-22: 97.20% (10,125 objects)\n",
      "mag_i 22-23: 96.16% (10,548 objects)\n",
      "mag_i 23-24: 94.13% (20,843 objects)\n",
      "mag_i 24-25: 90.29% (44,782 objects)\n",
      "mag_i 25-26: 82.54% (92,364 objects)\n",
      "mag_i 26-27: 67.77% (45,463 objects)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPLETENESS BY MAGNITUDE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "mag_comp = {}\n",
    "\n",
    "for ll in ['0.5', '1.0']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Linking Length {ll}\\\"\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    mag_comp[ll] = {}\n",
    "    # DD Completeness by Magnitude\n",
    "    print(f\"--- DeepDISC Completeness by Magnitude ---\")\n",
    "    dd_mag = calculate_comp_by_mag(all_matches[ll]['dd'], lsst_truth_cat, 'dd')\n",
    "    mag_comp[ll]['dd'] = dd_mag\n",
    "    \n",
    "    for _, row in dd_mag.iterrows():\n",
    "        print(f\"mag_i {row['mag_bin']}: {row['completeness']:.2%} ({row['n_total']:,} objects)\")\n",
    "    \n",
    "    # LSST Completeness by Magnitude\n",
    "    print(f\"\\n--- LSST Pipeline Completeness by Magnitude ---\")\n",
    "    lsst_mag = calculate_comp_by_mag(all_matches[ll]['lsst'], lsst_truth_cat, 'lsst')\n",
    "    mag_comp[ll]['lsst'] = lsst_mag\n",
    "    \n",
    "    for _, row in lsst_mag.iterrows():\n",
    "        print(f\"mag_i {row['mag_bin']}: {row['completeness']:.2%} ({row['n_total']:,} objects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f2d93-af0e-4d04-852b-6c89d5ade71c",
   "metadata": {},
   "source": [
    "# Completeness by Scenario\n",
    "Calculate completeness separately for isolated systems vs blended systems and break down blended systems by their classification outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95fca0f9-8f5b-41f9-a7d5-4517383e4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comp_by_scenario(object_matches, group_classifications, prefix='dd'):\n",
    "    \"\"\"\n",
    "    Calculate completeness separately for isolated vs blended systems.\n",
    "    \n",
    "    Args:\n",
    "        object_matches: DataFrame with object-level match records\n",
    "        group_classifications: DataFrame with group-level classifications\n",
    "        prefix: 'dd' or 'lsst'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Completeness statistics by scenario\n",
    "    \"\"\"\n",
    "    final_col = f'{prefix}_final'\n",
    "    # Filter to truth objects only\n",
    "    truth_matches = object_matches[object_matches['truth_row_idx'] != -1].copy()\n",
    "    # Drop duplicates to get unique truth objects\n",
    "    truth_matches_unique = truth_matches.drop_duplicates(subset='truth_row_idx')\n",
    "    # Join with group classifications\n",
    "    matches_with_groups = truth_matches_unique.merge(\n",
    "        group_classifications[[final_col, 'n_truth']],\n",
    "        left_on='group_id',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    results = {}\n",
    "    # Overall\n",
    "    results['overall'] = {\n",
    "        'n_total': len(matches_with_groups),\n",
    "        'n_matched': matches_with_groups['matched'].sum(),\n",
    "        'completeness': matches_with_groups['matched'].sum() / len(matches_with_groups)\n",
    "    }    \n",
    "    # Isolated (1 truth object)\n",
    "    isolated_mask = matches_with_groups['n_truth'] == 1\n",
    "    if isolated_mask.any():\n",
    "        isolated_data = matches_with_groups[isolated_mask]\n",
    "        results['isolated'] = {\n",
    "            'n_total': len(isolated_data),\n",
    "            'n_matched': isolated_data['matched'].sum(),\n",
    "            'completeness': isolated_data['matched'].sum() / len(isolated_data)\n",
    "        }\n",
    "    \n",
    "    # Blended (2+ truth objects)\n",
    "    blended_mask = matches_with_groups['n_truth'] >= 2\n",
    "    if blended_mask.any():\n",
    "        blended_data = matches_with_groups[blended_mask]\n",
    "        results['blended'] = {\n",
    "            'n_total': len(blended_data),\n",
    "            'n_matched': blended_data['matched'].sum(),\n",
    "            'completeness': blended_data['matched'].sum() / len(blended_data)\n",
    "        }\n",
    "    # Break down by blend outcome\n",
    "    blend_outcomes = ['resolved_blend', 'partial_deblend', 'unrec_blend', \n",
    "                     'shredded', 'unrec_blend_spurious']\n",
    "    \n",
    "    for outcome in blend_outcomes:\n",
    "        outcome_mask = matches_with_groups[final_col] == outcome\n",
    "        if outcome_mask.any():\n",
    "            outcome_data = matches_with_groups[outcome_mask]\n",
    "            results[outcome] = {\n",
    "                'n_total': len(outcome_data),\n",
    "                'n_matched': outcome_data['matched'].sum(),\n",
    "                'completeness': outcome_data['matched'].sum() / len(outcome_data)\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b668e4c-ed84-4662-958c-152c215814f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETENESS BY SCENARIO\n",
      "Linking Length 0.5\"\n",
      "--- DeepDISC Completeness by Scenario ---\n",
      "overall             : 81.41% (224,192 truth objects)\n",
      "isolated            : 83.50% (212,002 truth objects)\n",
      "blended             : 45.04% (12,190 truth objects)\n",
      "resolved_blend      : 100.00% (332 truth objects)\n",
      "partial_deblend     : 65.22% (46 truth objects)\n",
      "unrec_blend         : 49.39% (10,334 truth objects)\n",
      "shredded            : 100.00% (1,831 truth objects)\n",
      "unrec_blend_spurious: 50.00% (12 truth objects)\n",
      "\n",
      "--- LSST Pipeline Completeness by Scenario ---\n",
      "overall             : 82.82% (224,192 truth objects)\n",
      "isolated            : 84.86% (212,002 truth objects)\n",
      "blended             : 47.37% (12,190 truth objects)\n",
      "resolved_blend      : 100.00% (4 truth objects)\n",
      "unrec_blend         : 49.31% (11,703 truth objects)\n",
      "shredded            : 100.00% (34 truth objects)\n",
      "\n",
      "Linking Length 1.0\"\n",
      "--- DeepDISC Completeness by Scenario ---\n",
      "overall             : 88.05% (224,192 truth objects)\n",
      "isolated            : 92.15% (192,916 truth objects)\n",
      "blended             : 62.74% (31,276 truth objects)\n",
      "resolved_blend      : 100.00% (8,070 truth objects)\n",
      "partial_deblend     : 64.96% (2,049 truth objects)\n",
      "unrec_blend         : 48.55% (19,023 truth objects)\n",
      "shredded            : 100.00% (7,856 truth objects)\n",
      "unrec_blend_spurious: 50.00% (276 truth objects)\n",
      "\n",
      "--- LSST Pipeline Completeness by Scenario ---\n",
      "overall             : 83.47% (224,192 truth objects)\n",
      "isolated            : 88.91% (192,916 truth objects)\n",
      "blended             : 49.90% (31,276 truth objects)\n",
      "resolved_blend      : 100.00% (1,696 truth objects)\n",
      "partial_deblend     : 63.37% (830 truth objects)\n",
      "unrec_blend         : 47.99% (27,854 truth objects)\n",
      "shredded            : 100.00% (874 truth objects)\n",
      "unrec_blend_spurious: 46.15% (13 truth objects)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPLETENESS BY SCENARIO\")\n",
    "scenario_comp = {}\n",
    "\n",
    "for ll, counts_df in counts.items():\n",
    "    print(f\"Linking Length {ll}\\\"\")\n",
    "    scenario_comp[ll] = {}\n",
    "    # DD Completeness by Scenario\n",
    "    print(f\"--- DeepDISC Completeness by Scenario ---\")\n",
    "    dd_scenario = calculate_comp_by_scenario(all_matches[ll]['dd'], counts_df, 'dd')\n",
    "    scenario_comp[ll]['dd'] = dd_scenario\n",
    "    \n",
    "    for scenario, stats in dd_scenario.items():\n",
    "        print(f\"{scenario:20s}: {stats['completeness']:6.2%} ({stats['n_total']:,} truth objects)\")\n",
    "    # LSST Completeness by Scenario\n",
    "    print(f\"\\n--- LSST Pipeline Completeness by Scenario ---\")\n",
    "    lsst_scenario = calculate_comp_by_scenario(all_matches[ll]['lsst'], counts_df, 'lsst')\n",
    "    scenario_comp[ll]['lsst'] = lsst_scenario\n",
    "    \n",
    "    for scenario, stats in lsst_scenario.items():\n",
    "        print(f\"{scenario:20s}: {stats['completeness']:6.2%} ({stats['n_total']:,} truth objects)\")\n",
    "    print()\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c1081-6e0b-473f-a2ac-28a23db2a5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddbtknv-1118",
   "language": "python",
   "name": "ddbtknv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
